{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Prayag\n",
      "[nltk_data]     Chawla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Prayag\n",
      "[nltk_data]     Chawla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Prayag\n",
      "[nltk_data]     Chawla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Reviews.csv\",nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A3A63RACXR1XIL</td>\n",
       "      <td>A. Boodhoo \"deaddodo\"</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1204502400</td>\n",
       "      <td>constipation</td>\n",
       "      <td>we switched from the advance similac to the or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A5VVRGL8JA7R</td>\n",
       "      <td>Adam</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1306368000</td>\n",
       "      <td>Constipation Not A Problem if...</td>\n",
       "      <td>Like the bad reviews say, the organic formula ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A2TGDTJ8YCU6PD</td>\n",
       "      <td>geena77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1347494400</td>\n",
       "      <td>Love this formula!</td>\n",
       "      <td>I wanted to solely breastfeed but was unable t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>AUV4GIZZE693O</td>\n",
       "      <td>Susan Coe \"sueysis\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1203638400</td>\n",
       "      <td>very convenient</td>\n",
       "      <td>i love the fact that i can get this delieved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A82WIMR4RSVLI</td>\n",
       "      <td>Emrose mom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1337472000</td>\n",
       "      <td>The best weve tried so far</td>\n",
       "      <td>We have a 7 week old... He had gas and constip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id   ProductId          UserId                      ProfileName  \\\n",
       "0         1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1         2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2         3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3         4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4         5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...     ...         ...             ...                              ...   \n",
       "9995   9996  B000P41A28  A3A63RACXR1XIL            A. Boodhoo \"deaddodo\"   \n",
       "9996   9997  B000P41A28    A5VVRGL8JA7R                             Adam   \n",
       "9997   9998  B000P41A28  A2TGDTJ8YCU6PD                          geena77   \n",
       "9998   9999  B000P41A28   AUV4GIZZE693O              Susan Coe \"sueysis\"   \n",
       "9999  10000  B000P41A28   A82WIMR4RSVLI                       Emrose mom   \n",
       "\n",
       "      HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                        1                       1      5  1303862400   \n",
       "1                        0                       0      1  1346976000   \n",
       "2                        1                       1      4  1219017600   \n",
       "3                        3                       3      2  1307923200   \n",
       "4                        0                       0      5  1350777600   \n",
       "...                    ...                     ...    ...         ...   \n",
       "9995                    10                      15      1  1204502400   \n",
       "9996                     2                       3      5  1306368000   \n",
       "9997                     0                       0      5  1347494400   \n",
       "9998                     1                       2      5  1203638400   \n",
       "9999                     0                       1      4  1337472000   \n",
       "\n",
       "                               Summary  \\\n",
       "0                Good Quality Dog Food   \n",
       "1                    Not as Advertised   \n",
       "2                \"Delight\" says it all   \n",
       "3                       Cough Medicine   \n",
       "4                          Great taffy   \n",
       "...                                ...   \n",
       "9995                      constipation   \n",
       "9996  Constipation Not A Problem if...   \n",
       "9997                Love this formula!   \n",
       "9998                   very convenient   \n",
       "9999        The best weve tried so far   \n",
       "\n",
       "                                                   Text  \n",
       "0     I have bought several of the Vitality canned d...  \n",
       "1     Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2     This is a confection that has been around a fe...  \n",
       "3     If you are looking for the secret ingredient i...  \n",
       "4     Great taffy at a great price.  There was a wid...  \n",
       "...                                                 ...  \n",
       "9995  we switched from the advance similac to the or...  \n",
       "9996  Like the bad reviews say, the organic formula ...  \n",
       "9997  I wanted to solely breastfeed but was unable t...  \n",
       "9998  i love the fact that i can get this delieved t...  \n",
       "9999  We have a 7 week old... He had gas and constip...  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['Text'],inplace=True)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "input_data = df.loc[:,'Text']\n",
    "target_data = df.loc[:,'Summary']\n",
    "contractions=pickle.load(open(\"./contractions.pkl\",\"rb\"))['contractions']\n",
    "\n",
    "#initialize stop words and LancasterStemmer\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stemm=LancasterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A3A63RACXR1XIL</td>\n",
       "      <td>A. Boodhoo \"deaddodo\"</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1204502400</td>\n",
       "      <td>constipation</td>\n",
       "      <td>we switched from the advance similac to the or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A5VVRGL8JA7R</td>\n",
       "      <td>Adam</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1306368000</td>\n",
       "      <td>Constipation Not A Problem if...</td>\n",
       "      <td>Like the bad reviews say, the organic formula ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A2TGDTJ8YCU6PD</td>\n",
       "      <td>geena77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1347494400</td>\n",
       "      <td>Love this formula!</td>\n",
       "      <td>I wanted to solely breastfeed but was unable t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>AUV4GIZZE693O</td>\n",
       "      <td>Susan Coe \"sueysis\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1203638400</td>\n",
       "      <td>very convenient</td>\n",
       "      <td>i love the fact that i can get this delieved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A82WIMR4RSVLI</td>\n",
       "      <td>Emrose mom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1337472000</td>\n",
       "      <td>The best weve tried so far</td>\n",
       "      <td>We have a 7 week old... He had gas and constip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9513 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id   ProductId          UserId                      ProfileName  \\\n",
       "0         1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1         2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2         3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3         4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4         5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...     ...         ...             ...                              ...   \n",
       "9995   9996  B000P41A28  A3A63RACXR1XIL            A. Boodhoo \"deaddodo\"   \n",
       "9996   9997  B000P41A28    A5VVRGL8JA7R                             Adam   \n",
       "9997   9998  B000P41A28  A2TGDTJ8YCU6PD                          geena77   \n",
       "9998   9999  B000P41A28   AUV4GIZZE693O              Susan Coe \"sueysis\"   \n",
       "9999  10000  B000P41A28   A82WIMR4RSVLI                       Emrose mom   \n",
       "\n",
       "      HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                        1                       1      5  1303862400   \n",
       "1                        0                       0      1  1346976000   \n",
       "2                        1                       1      4  1219017600   \n",
       "3                        3                       3      2  1307923200   \n",
       "4                        0                       0      5  1350777600   \n",
       "...                    ...                     ...    ...         ...   \n",
       "9995                    10                      15      1  1204502400   \n",
       "9996                     2                       3      5  1306368000   \n",
       "9997                     0                       0      5  1347494400   \n",
       "9998                     1                       2      5  1203638400   \n",
       "9999                     0                       1      4  1337472000   \n",
       "\n",
       "                               Summary  \\\n",
       "0                Good Quality Dog Food   \n",
       "1                    Not as Advertised   \n",
       "2                \"Delight\" says it all   \n",
       "3                       Cough Medicine   \n",
       "4                          Great taffy   \n",
       "...                                ...   \n",
       "9995                      constipation   \n",
       "9996  Constipation Not A Problem if...   \n",
       "9997                Love this formula!   \n",
       "9998                   very convenient   \n",
       "9999        The best weve tried so far   \n",
       "\n",
       "                                                   Text  \n",
       "0     I have bought several of the Vitality canned d...  \n",
       "1     Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2     This is a confection that has been around a fe...  \n",
       "3     If you are looking for the secret ingredient i...  \n",
       "4     Great taffy at a great price.  There was a wid...  \n",
       "...                                                 ...  \n",
       "9995  we switched from the advance similac to the or...  \n",
       "9996  Like the bad reviews say, the organic formula ...  \n",
       "9997  I wanted to solely breastfeed but was unable t...  \n",
       "9998  i love the fact that i can get this delieved t...  \n",
       "9999  We have a 7 week old... He had gas and constip...  \n",
       "\n",
       "[9513 rows x 10 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_words=[]\n",
    "target_words=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions= pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stemm=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(texts,src):\n",
    "  #remove the html tags from the website\n",
    "  texts = BeautifulSoup(texts, \"lxml\").text\n",
    "  #tokenize the text into words \n",
    "  words=word_tokenize(texts.lower())\n",
    "  #filter words which contains \\ \n",
    "  #integers or their length is less than or equal to 3\n",
    "  words= list(filter(lambda w:(w.isalpha() and len(w)>=3),words))\n",
    "  words= [contractions[w] if w in contractions else w for w in words ]\n",
    "  \n",
    "  #stem the words to their root word and filter stop words\n",
    "  if src==\"inputs\":\n",
    "    words= [stemm.stem(w) for w in words if w not in stop_words]\n",
    "  else:\n",
    "    words= [w for w in words if w not in stop_words]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prayag Chawla\\AppData\\Roaming\\Python\\Python310\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#contraction file to expand shortened words\n",
    "for in_txt,tr_txt in zip(input_data,target_data):\n",
    "  in_words= clean(in_txt,\"inputs\")\n",
    "  input_texts+= [' '.join(in_words)]\n",
    "  input_words+= in_words\n",
    "  #add 'sos' at start and 'eos' at end of text\n",
    "  tr_words= clean(\"sos \"+tr_txt+\" eos\",\"target\")\n",
    "  target_texts+= [' '.join(tr_words)]\n",
    "  target_words+= tr_words\n",
    "\n",
    "# print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input words :  10353\n",
      "number of target words :  4173\n",
      "maximum input length :  73\n",
      "maximum target length :  19\n"
     ]
    }
   ],
   "source": [
    "input_words = sorted(list(set(input_words)))\n",
    "target_words = sorted(list(set(target_words)))\n",
    "num_in_words = len(input_words) #total number of input words\n",
    "num_tr_words = len(target_words) #total number of target words\n",
    "  \n",
    "max_in_len = mode([len(i) for i in input_texts])\n",
    "max_tr_len = mode([len(i) for i in target_texts])\n",
    " \n",
    "print(\"number of input words : \",num_in_words)\n",
    "print(\"number of target words : \",num_tr_words)\n",
    "print(\"maximum input length : \",max_in_len)\n",
    "print(\"maximum target length : \",max_tr_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tokenizer = Tokenizer()\n",
    "in_tokenizer.fit_on_texts(x_train)\n",
    "tr_tokenizer = Tokenizer()\n",
    "tr_tokenizer.fit_on_texts(y_train)\n",
    " \n",
    "#convert text into sequence of integers\n",
    "#where the integer will be the index of that word\n",
    "x_train= in_tokenizer.texts_to_sequences(x_train) \n",
    "y_train= tr_tokenizer.texts_to_sequences(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad array of 0's if the length is less than the maximum length \n",
    "en_in_data= pad_sequences(x_train,  maxlen=max_in_len, padding='post') \n",
    "dec_data= pad_sequences(y_train,  maxlen=max_tr_len, padding='post')\n",
    " \n",
    "#decoder input data will not include the last word \n",
    "#i.e. 'eos' in decoder input data\n",
    "dec_in_data = dec_data[:,:-1]\n",
    "#decoder target data will be one time step ahead as it will not include\n",
    "# the first word i.e 'sos'\n",
    "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session() \n",
    "latent_dim = 500\n",
    " \n",
    "#create input object of total number of encoder words\n",
    "en_inputs = Input(shape=(max_in_len,)) \n",
    "en_embedding = Embedding(num_in_words+1, latent_dim)(en_inputs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LSTM 1\n",
    "en_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "en_outputs1, state_h1, state_c1= en_lstm1(en_embedding) \n",
    " \n",
    "#LSTM2\n",
    "en_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "en_outputs2, state_h2, state_c2= en_lstm2(en_outputs1) \n",
    " \n",
    "#LSTM3\n",
    "en_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "en_outputs3 , state_h3 , state_c3= en_lstm3(en_outputs2)\n",
    " \n",
    "#encoder states\n",
    "en_states= [state_h3, state_c3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder. \n",
    "dec_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(num_tr_words+1, latent_dim) \n",
    "dec_embedding = dec_emb_layer(dec_inputs) \n",
    " \n",
    "#initialize decoder's LSTM layer with the output states of encoder\n",
    "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "dec_outputs, *_ = dec_lstm(dec_embedding,initial_state=en_states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention =Attention()\n",
    "attn_out = attention([dec_outputs,en_outputs3])\n",
    " \n",
    "#Concatenate the attention output with the decoder outputs\n",
    "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense layer (output layer)\n",
    "dec_dense = Dense(num_tr_words+1, activation='softmax') \n",
    "dec_outputs = dec_dense(merge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 73)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 73, 500)      5177000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 73, 500),    2002000     ['embedding[0][0]']              \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 73, 500),    2002000     ['lstm[0][0]']                   \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 500)    2087000     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 73, 500),    2002000     ['lstm_1[0][0]']                 \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 500),  2002000     ['embedding_1[0][0]',            \n",
      "                                 (None, 500),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 500)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, None, 500)    0           ['lstm_3[0][0]',                 \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " concat_layer1 (Concatenate)    (None, None, 1000)   0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 4174)   4178174     ['concat_layer1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,450,174\n",
      "Trainable params: 19,450,174\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "#Model class and model summary for text Summarizer\n",
    "model = Model([en_inputs, dec_inputs], dec_outputs) \n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 2724s 202s/step - loss: 2.3746 - accuracy: 0.7182 - val_loss: 1.2797 - val_accuracy: 0.8114\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 798s 57s/step - loss: 1.2491 - accuracy: 0.8200 - val_loss: 1.2438 - val_accuracy: 0.8318\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 687s 49s/step - loss: 1.1811 - accuracy: 0.8338 - val_loss: 1.1903 - val_accuracy: 0.8496\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 760s 53s/step - loss: 1.1154 - accuracy: 0.8509 - val_loss: 1.1677 - val_accuracy: 0.8519\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 2096s 158s/step - loss: 1.0918 - accuracy: 0.8534 - val_loss: 1.1706 - val_accuracy: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    }
   ],
   "source": [
    "model.compile( \n",
    "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) \n",
    "model.fit( \n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data, \n",
    "    batch_size=512, \n",
    "    epochs=5, \n",
    "    validation_split=0.1,\n",
    "    )\n",
    " \n",
    "\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim=500\n",
    "\n",
    "model = models.load_model(\"s2s\")\n",
    " \n",
    "#construct encoder model from the output of 6 layer i.e.last LSTM layer\n",
    "en_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
    "en_states=[state_h_enc,state_c_enc]\n",
    "#add input and state from the layer.\n",
    "en_model = Model(model.input[0],[en_outputs]+en_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoder inference\n",
    "# latent_dim=500\n",
    "# #/content/gdrive/MyDrive/Text Summarizer/\n",
    "# #load the model\n",
    "# model = models.load_model(\"s2s\")\n",
    " \n",
    "# #construct encoder model from the output of 6 layer i.e.last LSTM layer\n",
    "# en_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
    "# en_states=[state_h_enc,state_c_enc]\n",
    "# #add input and state from the layer.\n",
    "# en_model = Model(model.input[0],[en_outputs]+en_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder inference\n",
    "#create Input object for hidden and cell state for decoder\n",
    "#shape of layer with hidden or latent dimension\n",
    "dec_state_input_h = Input(shape=(latent_dim,))\n",
    "dec_state_input_c = Input(shape=(latent_dim,))\n",
    "dec_hidden_state_input = Input(shape=(max_in_len,latent_dim))\n",
    " \n",
    "# Get the embeddings and input layer from the model\n",
    "dec_inputs = model.input[1]\n",
    "dec_emb_layer = model.layers[5]\n",
    "dec_lstm = model.layers[7]\n",
    "dec_embedding= dec_emb_layer(dec_inputs)\n",
    " \n",
    "#add input and initialize LSTM layer with encoder LSTM states.\n",
    "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention layer\n",
    "attention = model.layers[8]\n",
    "attn_out2 = attention([dec_outputs2,dec_hidden_state_input])\n",
    " \n",
    "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense layer\n",
    "dec_dense = model.layers[10]\n",
    "dec_outputs2 = dec_dense(merge2)\n",
    "\n",
    "\n",
    " \n",
    "# Finally define the Model Class\n",
    "dec_model = Model(\n",
    "[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],\n",
    "[dec_outputs2] + [state_h2, state_c2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = tr_tokenizer.index_word\n",
    "reverse_source_word_index = in_tokenizer.index_word\n",
    "target_word_index = tr_tokenizer.word_index\n",
    "reverse_target_word_index[0]=' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    #get the encoder output and states by passing the input sequence\n",
    "    en_out, en_h, en_c= en_model.predict(input_seq)\n",
    "\n",
    "    #target sequence with inital word as 'sos'\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_word_index['sos']\n",
    "\n",
    "    #if the iteration reaches the end of text than it will be stop the iteration\n",
    "    stop_condition = False\n",
    "    #append every predicted word in decoded sentence\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: \n",
    "        #get predicted output, hidden and cell state.\n",
    "        output_words, dec_h, dec_c= dec_model.predict([target_seq] + [en_out,en_h, en_c])\n",
    "        \n",
    "        #get the index and from the dictionary get the word for that index.\n",
    "        word_index = np.argmax(output_words[0, -1, :])\n",
    "        text_word = reverse_target_word_index[word_index]\n",
    "        decoded_sentence += text_word +\" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find a stop word or last word.\n",
    "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
    "          stop_condition = True\n",
    "        \n",
    "        #update target sequence to the current word index.\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = word_index\n",
    "        en_h, en_c = dec_h, dec_c\n",
    "    \n",
    "    #return the deocded sentence\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exit condition: either hit max length\n",
    "#     # or find a stop word or last word.\n",
    "# if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
    "\n",
    "#     stop_condition = True\n",
    "#     #update target sequence to the current word index.\n",
    "# target_seq = np.zeros((1, 1))\n",
    "# target_seq[0, 0] = word_index\n",
    "# en_h, en_c = dec_h, dec_c\n",
    "# #return the decoded sentence\n",
    "# return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review : Driving down the small winding road that brings you to the beautiful Cala Moli, excitement was rising about the food we would be tasting this evening. Weâ€™d read about the collaboration between chefs Mauro Colagreco and Federico Desseno, both highly regarded cooks, with a shared passion for cooking over wood on an open fire.  We could smell a hint of the wood smoke from the kitchen as we parked outside El Silencio. Entering via the beautiful pool area, we passed the open kitchen where the chefs were already busy lighting the wood grill and preparing for service. We were immediately greeted by one of the restaurant management team who ushered us to our table at the front of the restaurant.  Silencio restaurant is positioned to the right of a small but very picturesque beach. An impressive structure for a feet-in-the-sand â€˜chiringuitoâ€™, although constructed from wood and natural materials, it has a feeling of meticulous elegance. The height of the building gives it a real sense of grandeur and beautiful grass blinds hanging from the ceiling are ready to drop to shade you when the intensity of the sun demands.  approached and introduced himself, proffering appetisers and drinks to keep us going while we perused the menu. Freshly baked flatbreads soon arrived, dotted with rosemary and glistening with olive oil, accompanied by thick cut, but amazingly succulent, Iberico ham.Silencioâ€™s menu is broken down into sections Crudo del Mar (raw seafood), Ensaladas (salads), Horno de Barro (clay oven), Brasas (grills), Pescados (fish), Carnes (meat) and Postres (desserts). After much discussion, we settled for Shrimp Aguachile, Tiraditos de Lubina and Oysters from the raw selection, a Burrata salad, baked aubergine from the oven, Carabineros from the grill and the turbot to share. We wisely decided to leave something from the meat section for another day! Opting for a refreshing bottle of Conreria dÂ´Scala Dei Les Brugueres, Priorat white wine to pair with our largely seafood selection, we sat back happy in the knowledge that the difficult part of choosing was over and now we could look forward to the tasting.\n",
      "\n",
      "Summarised review : driv smal wind road bring beauty cal mol excit ris food would tast ev read collab chef mauro colagreco federico desseno high regard cook shar pass cook wood op fir could smel hint wood smok kitch park outsid silencio ent via beauty pool are pass op kitch chef already busy light wood gril prep serv immedy greet on resta man team ush tabl front resta silencio resta posit right smal picturesqu beach impress structure chiringuito although construct wood nat mat feel metic eleg height build giv real sens grand beauty grass blind hang ceil ready drop shad intens sun demand approach introduc proff appet drink keep going perus menu fresh bak flatbread soon ar dot rosem glist ol oil accompany thick cut amaz succ iberico menu brok sect crudo del mar raw seafood ensalada salad horno barro clay ov brasa gril pescado fish carn meat post dessert much discuss settl shrimp aguachil tiradito lubin oyst raw select burrat salad bak aubergin ov carabinero gril turbot shar wis decid leav someth meat sect anoth day opt refresh bottl conrer dei les brugu pri whit win pair larg seafood select sat back happy knowledg difficult part choos could look forward tast\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[[ 105  163 2478  515  203 2616 3260 8750  544  156 2068  423  321  294\n",
      "  3523 4031  551 1441  399 2709  577 1500  462 5223  508 1172  510 4995\n",
      "   323  724  820   24 3394 1379 2107 3826  577  833  462  163  508 1172\n",
      "   563 1584  238  296  106  323 1441  208   69  384  433  182 5682 1950\n",
      "   262  522 1530  184 1500  833 1153  153  151 3693  613  334  840   55\n",
      "    50  690    2]]\n",
      "\n",
      " This is the final predicted summary\n",
      " great  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inp_review = \"\"\"Every Sharukh Khan release is an event in itself, RAAES is no different. Most of the characters the Khan had been portraying in the recent past have been disappointing to his fanseven the much admired Jahingar Khan of Hello Zindagi was not of much satisfaction due to the limited screen time. And now Raees is something his fans had been waiting and praying that it would tip the balance of fortune towards their much loved star\n",
    "\n",
    "# And yes.from the word go, we know a masterpiece is unfolding on the screen. Though the story is not new..a rise and fall of a righteous don has been done to death in Bollywood from times immemorial, but what sets Raees apart is Sharukh Khan, his performance and the amazing screenplay.\n",
    "\n",
    "# Jaideep Majumdar (Nawazuddin Siddiqui) reminiscences his tryst with bootlegger Raees (Sharukh Khan) way back in the 90's. Raees story unfolds right from his childhood where he is touchy about his short sight (has to wear glasses) and people calling him \"battery\" due to this impairment. His Ammi (Sheeba Chaddha) is his greatest influence and her words \"business is bigger than religion as long as it does not harm anyone\" remains in his mind and starts working for a local don (Atul Kulkarni).\n",
    "\n",
    "# Raees once when he is old enough decides to start his own business and miffs the don and joins hands with a Mumbai don Moosa who funds Raees's bootlegging business.\n",
    "\n",
    "# Raees falls in love and gets married to Aasiya (Mahira Khan) and soon becomes a father. His rise disturbs the local don who tries killing Raees and fails and ends up getting killed by him.\n",
    "\n",
    "# Jaideep Majumdar, a honest police officer is hot on the trail of Raees and uses every method at his disposal to bring Raees down, but he is unsuccessful as Raees has the support of both the ruling and the opposition party.\n",
    "\n",
    "# But soon, Raees's ego makes him a commit one fatal error which turns things in favor of Majumdar..\n",
    "\n",
    "# Sharukh is back to doing what he does best, hogging the screen time and also giving one of his career's best performances, his ire when called Battery, or when he is weeping away to glory in his wife's arms after a major setback or his astounding performance in the climax undoubtedly proves the crown is still his.\n",
    "\n",
    "# Mahira Khan performs well in what little screen time she has, as the only person who gets away after making fun of Raees's short sight. Wish she had more screen time.\"\"\"\n",
    "inp_review = input(\"Enter the input text : \")\n",
    "print(\"Review :\",inp_review)\n",
    "inp_review = clean(inp_review,\"inputs\")\n",
    "inp_review = ' '.join(inp_review)\n",
    "print(\"\\nSummarised review :\",inp_review)\n",
    "\n",
    "inp_x= in_tokenizer.texts_to_sequences([inp_review]) \n",
    "# print(inp_x)\n",
    "inp_x= pad_sequences(inp_x,  maxlen=max_in_len, padding='post')\n",
    "# print(inp_x)\n",
    " \n",
    "summary=decode_sequence(inp_x.reshape(1,max_in_len))\n",
    "print(inp_x.reshape(1,max_in_len))\n",
    "if 'eos' in summary :\n",
    "  summary=summary.replace('eos','')\n",
    "print(\"\\n This is the final predicted summary\\n\",summary);print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee74448ac26aec9e1f090088405c420048a8c66a68ad221e398cf15c498b3fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
